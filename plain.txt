数学之美

－、统计语言模型 ...2 二、谈谈中文分词 ...4 三、隐含马尔可夫模型在语言处理中的应用 .................7 四、数学之美系列四 — 怎样度量信息 ..9 五、简单之美：布尔代数和搜索引擎的索引 ...............11 六、图论和网络爬虫 (Web Crawlers).....14 七、信息论在信息处理中的应用 ............16 八、贾里尼克的故事和现代语言处理 ....18 九、如何确定网页和查询的相关性 ........21 十、有限状态机和地址识别 ....................24 十一、 Google 阿卡 47 的制造者阿米特 . 辛格博士 ....26 十二、余弦定理和新闻的分类 ................28 十三、信息指纹及其应用 .31 十四、谈谈数学模型的重要性 ................33 十五、繁与简 自然语言处理的几位精英 .....................36 十六、不要把所有的鸡蛋放在一个篮子里 -- 谈谈最大熵模型 （上） ...............38 十六、不要把所有的鸡蛋放在一个篮子里 最大熵模型 （下） ..40 十七、闪光的不一定是金子 谈谈搜索引擎作弊问题 (Search Engine Anti-SPAM)..42 十八、矩阵运算和文本处理中的分类问题 ...................44 十九、马尔可夫链的扩展 贝叶斯网络 (Bayesian Networks) ............47 二十、自然语言处理的教父 马库斯 ......49 二十一、布隆过滤器（ Bloom Filter ） ...51 二十二、谈谈密码学的数学原理 ............53 二十三、输入一个汉字需要敲多少个键 — 谈谈香农第一定律 ......57

摘自互联网 : http://harryxu.cn/blog/google_math 整理人 : 心灯 ( bjbs27@hotmail.com )

 

－、统计语言模型

2006 年 4 月 3 日 上午 08:15:00

从本周开始，我们将定期刊登 Google 科学家吴军写的《数学之美》系列文章， 介绍数学在信息检索和自然语言处理中的主导作用和奇妙应用。

发表者: 吴军, Google 研究员

前言

也 许大家不相信，数学是解决信息检索和自然语言处理的最好工具。它能非常 清晰地描述这些领域的实际问题并且给出漂亮的解决办法。每当人们应用数学工 具解决一 个语言问题时，总会感叹数学之美。我们希望利用 Google 中文黑板 报这块园地，介绍一些数学工具，以及我们是如何利用这些工具来开发 Google 产品的。

系列一： 统计语言模型 (Statistical Language Models)

Google 的使命是整合全球的信息，所以我们一直致力于研究如何让机器对信息、 语言做最好的理解和处理。长期以来，人类一直梦想着能让机器代替人来翻译语 言、识别语 音、认识文字（不论是印刷体或手写体）和进行海量文献的自动检 索，这就需要让机器理解语言。但是人类的语言可以说是信息里最复杂最动态的 一部分。为了解决 这个问题，人们容易想到的办法就是让机器模拟人类进行学 习 - 学习人类的语法、分析语句等等。尤其是在乔姆斯基（Noam Chomsky 有史 以来最伟大的语言学家）提出 “形式语言” 以后，人们更坚定了利用语法规则 的办法进行文字处理的信念。遗憾的是，几十年过去了，在计算机处理语言领域， 基于这个语法规则的方法几乎毫无突破。

其实早在几十年前，数学家兼信息论的祖师爷 香农 (Claude Shannon)就提出了 用数学的办法处理自然语言的想法。遗憾的是当时的计算机条件根本无法满足大 量信息处理的需要，所以他这个想法当时并没有被人们重视。七十年代初，有了 大规模集成电路的快速计算机后，香农的梦想才得以实现。

首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师贾里尼 克 ( Fred Jelinek ) 。 当 时 贾 里 尼 克 在 IBM 公 司 做 学 术 休 假 (Sabbatical Leave)，领导了一批杰出的科学家利用大型计算机来处理人类语言问题。统计语 言模型就是在那个时候提出的。

给大家举个例子：在很多涉及到自然语言处理的领域，如机器翻译、语音识别、 印刷体或手写体识别、拼写纠错、汉字输入和文献查询中，我们都需要知道一个 文字序列是否能构成一个大家能理解的句子，显示给使用者。对这个问题，我们 可以用一个简单的统计模型来解决这个问题。

如 果 S 表示一连串特定顺序排列的词 w1， w2，…， wn ，换句话说，S 可以 表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子。现在，机器 对语言的识别从某种角度来说，就是想知道S在文本中出现的可能性，也就是数 学上所说的S 的概率用 P(S) 来表示。利用条件概率的公式，S 这个序列出现的 概率等于每一个词出现的概率相乘，于是P(S) 可展开为：

P(S) = P(w1)P(w2|w1)P(w3| w1 w2)…P(wn|w1 w2…wn-1)

其 中 P (w1) 表示第一个词w1 出现的概率；P (w2|w1) 是在已知第一个词的前 提下，第二个词出现的概率；以次类推。不难看出，到了词wn，它的出现概率取 决于它前面所有词。从计算上来看，各种可能性太多，无法 实现。因此我们假 定任意一个词wi的出现概率只同它前面的词 wi-1 有关(即马尔可夫假设），于 是问题就变得很简单了。现在，S 出现的概率就变为：

P(S) = P(w1)P(w2|w1)P(w3|w2)…P(wi|wi-1)…

(当然，也可以假设一个词又前面N-1 个词决定，模型稍微复杂些。）

接 下来的问题就是如何估计 P (wi|wi-1)。现在有了大量机读文本后，这个问 题变得很简单，只要数一数这对词（wi-1,wi) 在统计的文本中出现了多少次， 以及 wi-1 本身在同样的文本中前后相邻出现了多少次，然后用两个数一除就可 以了,P(wi|wi-1) = P(wi-1,wi)/ P (wi-1)。

也许很多人不相信用这么简单的数学模型能解决复杂的语音识别、机器翻译等问 题。其实不光是常人，就连很多语言学家都曾质疑过这种方法的有效性，但事实 证明，统计语言模型比任何已知的借助某种规则的解决方法都有效。比如在 Google 的 中英文自动翻译 中，用的最重要的就是这个统计语言模型。去年美国 标准局(NIST) 对所有的机器翻译系统进行了评测，Google 的系统是不仅是全世 界最好的，而且高出所有基于规则的系统很多。

现 在，读者也许已经能感受到数学的美妙之处了，它把一些复杂的问题变得如 此的简单。当然，真正实现一个好的统计语言模型还有许多细节问题需要解决。 贾里尼克 和他的同事的贡献在于提出了统计语言模型，而且很漂亮地解决了所 有的细节问题。十几年后，李开复用统计语言模型把 997 词语音识别的问题简 化成了一个 20 词的识别问题，实现了有史以来第一次大词汇量非特定人连续语 音的识别。

我是一名科学研究人员 ，我在工作中经常惊叹于数学语言应用于解决实际问题 上时的神奇。我也希望把这种神奇讲解给大家听。当然，归根结底，不管什莫样 的科学方法、无论多莫奇妙的解决手段都是为人服务的。我希望 Google 多努力 一分，用户就多一分搜索的喜悦



二、谈谈中文分词

2006 年 4 月 10 日 上午 08:10:00

发表者: 吴军， Google 研究员

谈谈中文分词

----- 统计语言模型在中文处理中的一个应用

上回我们谈到 利用统计语言模型进行语言处理 ，由于模型是建立在词的基础上 的，对于中日韩等语言，首先需要进行分词。例如把句子 “中国航天官员应邀 到美国与太空总署官员开会。”

分成一串词：

中国 / 航天 / 官员 / 应邀 / 到 / 美国 / 与 / 太空 / 总署 / 官员 / 开 会。

最容易想到的，也是最简单的分词办法就是查字典。这种方法最早是由北京航天 航空大学的梁南元教授提出的。

用 “查字典” 法，其实就是我们把一个句子从左向右扫描一遍，遇到字典里有 的词就标识出来，遇到复合词（比如 “上海大学”）就找最长的词匹配，遇到 不认识的字串就分割成单字词，于是简单的分词就完成了。这种简单的分词方法 完全能处理上面例子中的句子。八十年代， 哈工大的王晓龙博士 把 它理论化， 发展成最少词数的分词理论，即一句话应该分成数量最少的词串。这种方法一个 明显的不足是当遇到有二义性 （有双重理解意思）的分割时就无能为力了。比 如，对短语 “发展中国家” 正确的分割是“发展-中-国家”，而从左向右查字 典的办法会将它分割成“发展-中国-家”，显然是错了。另外，并非所有的最长 匹配都一定是正确的。比如 “上海大学城书店”的正确分词应该是 “上海-大 学城-书店，” 而不是 “上海大学-城-书店”。

九十年代以前，海内外不少学者试图用一些文法规则来解决分词的二义性问题， 都不是很成功。90 年前后，清华大学的郭进博士用统计语言模型成功解决分词 二义性问题，将汉语分词的错误率降低了一个数量级。

利用统计语言模型分词的方法，可以用几个数学公式简单概括如下：

我们假定一个句子S可以有几种分词方法，为了简单起见我们假定有以下三种： A1, A2, A3, ..., Ak, B1, B2, B3, ..., Bm C1, C2, C3, ..., Cn

其中，A1, A2, B1, B2, C1, C2 等等都是汉语的词。那么最好的一种分词方法 应该保证分完词后这个句子出现的概率最大。也就是说如果 A1,A2,..., Ak 是最好的分法，那么 （P 表示概率）：

P (A1, A2, A3, ..., Ak） 〉 P (B1, B2, B3, ..., Bm), 并且 P (A1, A2, A3, ..., Ak） 〉 P(C1, C2, C3, ..., Cn)

因此，只要我们利用上回提到的统计语言模型计算出每种分词后句子出现的概 率，并找出其中概率最大的，我们就能够找到最好的分词方法。

当然，这里面有一个实现的技巧。如果我们穷举所有可能的分词方法并计算出每 种可能性下句子的概率，那么计算量是相当大的。因此，我们可以把它看成是一 个 动态规划 （Dynamic Programming) 的问题，并利用 “维特比”（ Viterbi ） 算 法快速地找到最佳分词。

在清华大学的郭进博士以后，海内外不少学者利用统计的方法，进一步完善中文 分词。其中值得一提的是清华大学孙茂松教授和香港科技大学吴德凯教授的工 作。

需 要指出的是，语言学家对词语的定义不完全相同。比如说 “北京大学”，有 人认为是一个词，而有人认为该分成两个词。一个折中的解决办法是在分词的同 时，找到复合词的嵌套结构。在上面的例子中，如果一句话包含 “北京大学” 四个字，那么先把它当成一个四字词，然后再进一步找出细分词 “北京” 和 “大学”。这种方法是最早是郭进在 “Computational Linguistics” （《计 算机语言学》）杂志上发表的，以后不少系统采用这种方法。

一般来讲，根 据不同应用，汉语分词的颗粒度大小应该不同。比如，在机器翻 译中，颗粒度应该大一些，“北京大学”就不能被分成两个词。而在语音识别中， “北京大学”一般 是被分成两个词。因此，不同的应用，应该有不同的分词系 统。Google 的葛显平博士和朱安博士，专门为搜索设计和实现了自己的分词系 统。

也 许你想不到，中文分词的方法也被应用到英语处理，主要是手写体识别中。 因为在识别手写体时，单词之间的空格就不很清楚了。中文分词方法可以帮助判 别英语单 词的边界。其实，语言处理的许多数学方法通用的和具体的语言无关。 在 Google 内，我们在设计语言处理的算法时，都会考虑它是否能很容易地适用 于各种自然语言。这样，我们才能有效地支持上百种语言的搜索。

对中文分词有兴趣的读者，可以阅读以下文献：

1. 梁南元

书面汉语自动分词系统

http://www.touchwrite.com/demo/LiangNanyuan-JCIP-1987.pdf

2. 郭进

统计语言模型和汉语音字转换的一些新结果

http://www.touchwrite.com/demo/GuoJin-JCIP-1993.pdf

3. 郭进

Critical Tokenization and its Properties http://acl.ldc.upenn.edu/J/J97/J97-4004.pdf

4. 孙茂松

Chinese word segmentation without using lexicon and hand-crafted training data

http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=980775

三、隐含马尔可夫模型在语言处理中的应用
